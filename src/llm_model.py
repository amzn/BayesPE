from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch
import numpy as np
import scipy

class LLM(object):
  """
  Class for native LLM from Huggingface
  """

  def __init__(self, model_name="tiiuae/falcon-7b-instruct", temperature=0.7, max_tokens=200, use_pipeline=False, use_reduced_precision=False):
    """
    :param model_name: llm to use
    :param temperature: temperature for generation (only relevant for text generation).
    :param max_tokens: maximum number of generated tokens (only relevant for text generation).
    :param use_pipeline: whether to use transformers pipeline for generation (only if you have to generate text, for prompting pipeline is not an option).
    :param use_reduced_precision: whether to use reduced precision for the LLM to use less GPU memory and compute.
    """

    self.model_name = model_name
    self.temperature = temperature
    self.max_tokens = max_tokens
    self.use_pipeline = use_pipeline
    self.num_results = 1
    self.softmax = torch.nn.Softmax(dim=2)

    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
    if use_pipeline:
        self.pipeline = transformers.pipeline(
            "text-generation",
            model=self.model_name,
            tokenizer=self.tokenizer,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto",
        )
    else:
        if use_reduced_precision:
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, trust_remote_code=True, device_map='auto', torch_dtype = torch.bfloat16)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, trust_remote_code=True, device_map='auto')

        self.generate_kwargs = {
            "temperature": temperature,
            "max_new_tokens": max_tokens,
            "use_cache": True,
            "do_sample": True,
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": self.tokenizer.pad_token_id,
        }

  def generate_text(self, instructions, content):
      """
      Generate text with the LLM.

      :param instructions: text string for instruction portion of the prompt.
      :param content: text string for content portion of the prompt.

      :return: text string generated by the model.
      """
      prompt_text = '''{}

{} 
'''.format(instructions, content)
      if self.use_pipeline:
          sequences = self.pipeline(
          prompt_text,
          do_sample=True,
          top_k=10,
          num_return_sequences=1,
          eos_token_id=self.tokenizer.eos_token_id,
          pad_token_id=self.tokenizer.eos_token_id,
          max_length=self.max_tokens,
          temperature=self.temperature
          )
          return sequences[0]['generated_text'][len(prompt_text):]
      else:
          indexed_tokens = self.tokenizer.encode(prompt_text)
          # Convert indexed tokens in a PyTorch tensor
          tokens_tensor = torch.tensor([indexed_tokens])
          with torch.no_grad():
              outputs = self.model.generate(tokens_tensor, **self.generate_kwargs)
              predictions = outputs[0]
          return self.tokenizer.decode(predictions)[len(prompt_text):]

  def return_logits(self, instructions, content=None):
      """
      return the logits (p(word) = C*exp(logit(word))) over the vocabulary for the first new token.

      :param instructions: text string for instruction portion of the prompt.
      :param content: text string for content portion of the prompt.

      :return: logits at the last token [n_vocabulary].
      """
      if content is not None:
          prompt_text = '''{}
{}'''.format(instructions, content)
      else:
          prompt_text = '''{}'''.format(instructions)
      # print(prompt_text)
      indexed_tokens = self.tokenizer.encode(prompt_text)
      # Convert indexed tokens in a PyTorch tensor
      tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')
      with torch.no_grad():
          outputs = self.model(tokens_tensor)
      return outputs.logits[0, -1, :]

  def word_logit(self, logits, word):
      """
      given logits over the vocabulary and a word, it returns the logit for the word.

      :param logits: logits over the vocabulary [n_vocabulary].
      :param word: string containing the word to index (if multiple words given, it uses the first one).

      :return: logit(word).
      """
      if self.model_name.startswith('mistralai') or self.model_name.startswith('google'):
          idx = self.tokenizer.encode(word)[1]
      else:
          idx = self.tokenizer.encode(word)[0]
      return logits[idx].cpu().detach().numpy()

  def class_probabilities(self, instructions, content, class_words):
      """
      Get class probabilities given classes key words

      :param instructions: text string for instruction portion of the prompt.
      :param content: text string for content portion of the prompt.
      :param class_words: list of words for each class.

      :return: 1D vector of class probabilities [n_classes].
      """
      logits = self.return_logits(instructions, content)
      class_logits = np.zeros(len(class_words))
      for i in range(len(class_words)):
          class_logits[i] = self.word_logit(logits, class_words[i])
      unnorm_probs = scipy.special.softmax(class_logits)
      return np.divide(unnorm_probs, np.sum(unnorm_probs))
